# Extreme Language Model Compression with Optimal Subwords and Shared Projections

## Paper

- link : https://arxiv.org/abs/1909.11687

- 키워드 : BERT, language model,  Knowledge distillation, model compression

- 한줄 소개 : BERT를 Knowledge distillation을 통해 압축

### References

- https://medium.com/huggingface/distilbert-8cf3380435b5
- https://towardsdatascience.com/distilling-bert-how-to-achieve-bert-performance-using-logistic-regression-69a7fc14249d