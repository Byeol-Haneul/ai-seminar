# BERT

## Pre-training of Deep Bidirectional Transformers for Language Understanding[[pdf](https://arxiv.org/pdf/1810.04805.pdf)]


* Author
  * Jacob Devlin (Google AI Language)
  * Ming-Wei Chang (Google AI Language)
  * Kenton Lee (Google AI Language)
  * Kristina Toutanova (Google AI Language)


### 참고문헌

* OpenAI GPT Paper [[link]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
* BERT Explained [[link]](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
* Clova AI 서민준 님의 BERT 17분 분석 동영상 [[link]](https://www.facebook.com/groups/TensorFlowKR/permalink/768188583522202/)
* BERT Github [[link]](https://github.com/google-research/bert)

