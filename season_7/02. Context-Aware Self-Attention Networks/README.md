# Context-Aware Self-Attention Networks

## Abstract
self attention, context aware self attention, NMT

## References
논문

https://arxiv.org/pdf/1902.05766.pdf

https://arxiv.org/abs/1706.03762.pdf

설명

https://pozalabs.github.io/transformer/

https://www.slideshare.net/JunhoLee124/attention-is-all-you-need-113895030
http://blog.naver.com/PostView.nhn?blogId=hist0134&logNo=221035988217&redirect=Dlog&widgetTypeCall=true

PR-12 영상 (attention is all you need)

https://www.youtube.com/watch?v=6zGgVIlStXs