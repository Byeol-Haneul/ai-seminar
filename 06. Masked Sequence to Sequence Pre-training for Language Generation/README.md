# Masked Sequence to Sequence Pre-training for Language Generation

## Paper

- link https://arxiv.org/pdf/1905.02450.pdf

- 키워드 : masked language model, sequence to sequence, neural machine translation

- 한줄 소개 : BERT를 확장한 masked language model for natural language generation

### References

- 
