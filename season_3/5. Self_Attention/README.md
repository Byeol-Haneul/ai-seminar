# Self Attention

## 1. A Structured Self-attentive Sentence Embedding
* 


## 2. Attention is All You Need
* [논문(arxiv)](https://arxiv.org/abs/1706.03762)
* [가장 잘 설명된 블로그](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.W2Fx-tgzbzg)
* [구글 아티클](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
* [한글 리뷰 1](https://github.com/YBIGTA/DeepNLP-Study/wiki/Attention-Is-All-You-Need-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0)
* [한글 리뷰 2](http://dalpo0814.tistory.com/49)
* [구현 코드 Kyubyong님 github](https://github.com/Kyubyong/transformer)
* [구현 코드 DongjunLee님 github](https://github.com/DongjunLee/transformer-tensorflow)
* [tensor2tensor](https://github.com/tensorflow/tensor2tensor) : 구글에서 공개한 transformer 등의 tensor2tensor 모델 패키지
