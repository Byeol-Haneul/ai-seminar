# Word2Vec
###### By Jumin Oh, Doosoo Chang
### Before Reading
- This document is written for studying in advance.
- Not all contents of following document will be quoted to presentation.
- I want you to read light-hearted. :)
## Core idea and Definition
- Word2Vec 기저논문 한국어 번역본 : https://medium.com/mathpresso/efficient-estimation-of-word-representations-in-vector-space-%EB%B2%88%EC%97%AD-ac2a104a23ca
- Word2Vec 논문 간략 정리본 : http://nohhj.blogspot.kr/2015/08/word-embedding.html
- Word2Vec 개요 : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/11/embedding/
- Word2Vec 응용 : https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/
- Word2Vec 관련 이론 정리 : https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/
- Word2Vec 학습 방식 : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/

##TensorFlow Example

- TensorFlowKorea Word2Vec Tutorial 번역 : https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/word2vec/

## Applications

- Word2Vec를 이용한 추천 시스템 : https://brunch.co.kr/@goodvc78/16


## Papers

[1]  T. Mikolov, et al., Distributed Representations of Words and Phrases.

[2]  X. Rong, word2vec Parameter Learning Explained.

[3]  Y. Goldberg, et al., word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding 
​      Method.