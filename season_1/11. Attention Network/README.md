# Attention Network 예습자료

## 발표 내용

* Attention 개념, seq2seq의 enc-dec에서의 변경점, alignment model 등의 용어와 개념 정리
* Global attention vs local attention
* Image captioning 등에서의 응용 분야 소개
* 간단한 코드 설명

## Reference

* Neural Machine Translation by Jointly Learning to Align and Translate(ICLR 2015), Bahdanau et al. [[paper]](https://arxiv.org/abs/1409.0473)
	* NMT에서 attention mechanism을 제안
* Effective Approaches to Attention-based Neural Machine Translation(EMNLP 2015), Luong et al. [[paper]](https://arxiv.org/abs/1508.04025)
	* Local attention의 개념으로 계산속도 향상
* Attention mechanism에 대해 한글로 개념을 간단하게 잘 설명한 블로그(한글) [[블로그]](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/10/06/attention/)
* Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (ICML 2015), Xu et al. [[paper]](https://arxiv.org/abs/1502.03044)
	* Image captioning에 attention을 접목한 논문
	* Soft & hard attention
* Soft & hard attention에 대한 설명(영어) [[blog]](https://jhui.github.io/2017/03/15/Soft-and-hard-attention/)
