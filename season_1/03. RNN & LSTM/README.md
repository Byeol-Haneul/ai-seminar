Welcome to RNN & LSTM!
===================

----------


예습자료 링크 및 출처
-------------

> <i class="icon-file"></i> **Note:**

> - RNN 기본구조 : 
https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/
> - RNN의 다양한 구조 :
https://jay.tech.blog/2016/12/07/vanilla-rnn/
> - LSTM 유닛과 게이트 작동방식 : 
> https://deeplearning4j.org/kr/lstm
> - Cross entropy in RNN
> http://funmv2013.blogspot.kr/2017/01/cross-entropy.html

12월 7일 목차
-------------
> - RNN 등장배경 :
> - RNN 정의 :
> - RNN 특성 : 
> - RNN 그림 및 구조 :
> - RNN in TensorFlow : 
> - RNN in TensorFlow Traning : 
> - RNN with Long Sequences :
> - Stacked RNN :
> - Softmax (FC) in Deep CNN : 
> - CNN + RNN
> - Tutorial : Image Captioning
> - 시연동영상



The Image Captioning Model (a CNN followed by RNN)
 -------------
 Update (September 22, 2016): The Google Brain team has released the image captioning model of Vinyals et al. (2015). The core model is very similar to NeuralTalk2 (a CNN followed by RNN), but the Google release should work significantly better as a result of better CNN, some tricks, and more careful engineering. Find it under im2txt repo in tensorflow. I'll leave this code base up for educational purposes and as a Torch implementation.
 
 https://github.com/karpathy/neuraltalk2
 
 Image Captioning 시뮬레이션 링크 :
 http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/
 
 Automated Image Captioning with ConvNets and Recurrent Nets
  -------------
  https://cs.stanford.edu/people/karpathy/sfmltalk.pdf
 

CNN + RNN 논문 
-------------
https://arxiv.org/pdf/1507.05717v1.pdf


Minimal character-level Vanilla RNN model. 
 ------------
https://gist.github.com/karpathy/d4dee566867f8291f086


LSTM_loss  
 ------------
https://gist.github.com/ratsgo/6e9a094c7108dee8147ef0a13666de47#file-lstm_loss-py

Q&A 
 ------------


